#!/usr/bin/env python3
"""
Comprehensive Statistical Analysis and Score Calculation
for Android Architecture Benchmarks (6 Architectures - Including HYBRID)
"""

import json
import os
import numpy as np
from scipy import stats
from scipy.stats import friedmanchisquare, rankdata
import scikit_posthocs as sp
import pandas as pd
from itertools import combinations
from datetime import datetime
from collections import defaultdict

# ============ KONFIGÃœRASYON ============
ARCH_MAPPING = {
    "classicmvvm": "Classic MVVM",
    "mvc": "MVC",
    "mvi": "MVI",
    "mvp": "MVP",
    "singlestatemvvm": "Single-State MVVM",
    "hybrid": "HYBRID"
}

ALL_TESTS = [
    "startupCold", "startupWarm",
    "productListScrollAndPagination", "productListRapidScrolling", "productListCategoryFiltering",
    "cartQuantityUpdatesWithDynamicSetup", "cartCheckoutFlow",
    "chatListRealtimeUpdates", "chatDetailMessageStreamAndSending", "chatRapidSwitching",
    "continuousScrollJankTest", "flingJankTest", "rapidDirectionChangeJankTest",
    "cartQuantityUpdatePerformance", "categoryFilterPerformance"
]

HIGHER_IS_BETTER = {
    "startupCold": False,
    "startupWarm": False,
    "productListScrollAndPagination": True,
    "productListRapidScrolling": True,
    "productListCategoryFiltering": True,
    "cartQuantityUpdatesWithDynamicSetup": True,
    "cartCheckoutFlow": True,
    "chatListRealtimeUpdates": True,
    "chatDetailMessageStreamAndSending": True,
    "chatRapidSwitching": True,
    "continuousScrollJankTest": True,
    "flingJankTest": True,
    "rapidDirectionChangeJankTest": True,
    "cartQuantityUpdatePerformance": True,
    "categoryFilterPerformance": True
}

DATA_DIR = "rawdata/performance"
OUTPUT_FILE = "analysis_result/comprehensive_analysis_6arch.json"

# ============ VERÄ° YÃœKLEME ============
def get_metric_name(test_name):
    """Determine metric name for a test"""
    if test_name in ["startupCold", "startupWarm"]:
        return "timeToInitialDisplayMs"
    else:
        return "frameCount"

def get_runs_from_benchmark(benchmark, metric_name):
    """Extract runs data from benchmark"""
    # First try: metrics.[metricName].runs
    if "metrics" in benchmark and metric_name in benchmark["metrics"]:
        if "runs" in benchmark["metrics"][metric_name]:
            return benchmark["metrics"][metric_name]["runs"]
    
    # Could also check runs_detail here if needed
    # For now, we rely on metrics.runs
    
    return None

def load_benchmark_data():
    """Load all benchmark data from JSON files"""
    data = {}
    
    json_files = {
        "classicmvvm": "classicmvvm_result.json",
        "mvc": "mvc_result.json",
        "mvi": "mvi_result.json",
        "mvp": "mvp_result.json",
        "singlestatemvvm": "singlestatemvvm_result.json",
        "hybrid": "hybrid_result.json"
    }
    
    for arch_key, filename in json_files.items():
        filepath = os.path.join(DATA_DIR, filename)
        if not os.path.exists(filepath):
            print(f"Warning: {filename} not found, skipping {arch_key}")
            continue
        
        with open(filepath, 'r') as f:
            arch_data = json.load(f)
        
        arch_name = ARCH_MAPPING[arch_key]
        data[arch_name] = {}
        
        if "benchmarks" not in arch_data:
            print(f"Warning: No 'benchmarks' in {filename}")
            continue
        
        for benchmark in arch_data["benchmarks"]:
            test_name = benchmark.get("name")
            if test_name not in ALL_TESTS:
                continue
            
            metric_name = get_metric_name(test_name)
            runs = get_runs_from_benchmark(benchmark, metric_name)
            
            if runs is None or len(runs) == 0:
                continue
            
            data[arch_name][test_name] = runs
    
    return data

def parse_memory_from_json(filepath):
    """Parse memory data from JSON file"""
    with open(filepath, 'r') as f:
        data = json.load(f)
    
    if "memoryBenchmarkResult" in data and len(data["memoryBenchmarkResult"]) > 0:
        totalpss_row = None
        for row in data["memoryBenchmarkResult"]:
            if row.get("Label") == "TotalPSS_MB":
                totalpss_row = row
                break
        
        if totalpss_row:
            initial = totalpss_row.get("01_AppLaunch")
            if initial is None:
                return None
            
            peak = initial
            for key, value in totalpss_row.items():
                if key.startswith(("0", "1")) and isinstance(value, (int, float)) and value > peak:
                    peak = value
            
            final = totalpss_row.get("10_Peak_AfterAllOperations") or totalpss_row.get("09_ChatDetail_AfterStream")
            if final is None:
                final = initial
            
            growth = final - initial
            
            return {"initial": float(initial), "peak": float(peak), "growth": float(growth)}
    
    return None

def load_memory_data():
    """Load memory data from JSON files"""
    memory_data = {}
    
    json_files = {
        "classicmvvm": "classicmvvm_result.json",
        "mvc": "mvc_result.json",
        "mvi": "mvi_result.json",
        "mvp": "mvp_result.json",
        "singlestatemvvm": "singlestatemvvm_result.json",
        "hybrid": "hybrid_result.json"
    }
    
    # Known memory values (non-hybrid)
    known_values = {
        "Classic MVVM": {"initial": 30.40, "peak": 38.42, "growth": 0.55},
        "MVC": {"initial": 30.34, "peak": 37.71, "growth": 0.48},
        "MVI": {"initial": 29.57, "peak": 38.42, "growth": 1.09},
        "MVP": {"initial": 30.00, "peak": 38.00, "growth": 1.00},
        "Single-State MVVM": {"initial": 33.28, "peak": 40.87, "growth": 0.37}
    }
    
    for arch_key, filename in json_files.items():
        arch_name = ARCH_MAPPING[arch_key]
        filepath = os.path.join(DATA_DIR, filename)
        
        if arch_name in known_values:
            memory_data[arch_name] = known_values[arch_name]
        elif os.path.exists(filepath):
            parsed = parse_memory_from_json(filepath)
            if parsed:
                memory_data[arch_name] = parsed
            else:
                print(f"Warning: Could not parse memory data for {arch_name}")
    
    return memory_data

def load_code_quality_data():
    """Load code quality data (HYBRID has no static analysis)"""
    code_data = {
        "Classic MVVM": {"sloc": 2459, "debt_hours": 2.9, "avg_cog": 2.61, "avg_cyc": 1.95, "max_cyc": 9},
        "MVC": {"sloc": 2715, "debt_hours": 1.7, "avg_cog": 1.83, "avg_cyc": 1.75, "max_cyc": 9},
        "MVI": {"sloc": 2510, "debt_hours": 2.0, "avg_cog": 2.60, "avg_cyc": 2.38, "max_cyc": 9},
        "MVP": {"sloc": 3001, "debt_hours": 4.6, "avg_cog": 1.50, "avg_cyc": 1.61, "max_cyc": 10},
        "Single-State MVVM": {"sloc": 2442, "debt_hours": 2.0, "avg_cog": 2.50, "avg_cyc": 2.02, "max_cyc": 14},
        "HYBRID": None  # No static analysis data
    }
    return code_data

# ============ Ä°STATÄ°STÄ°KSEL ANALÄ°Z ============
def calculate_descriptive_stats(values):
    """Calculate descriptive statistics"""
    if len(values) == 0:
        return {"mean": 0, "median": 0, "std": 0, "cv_percent": 0, "min": 0, "max": 0, "n": 0}
    
    values_arr = np.array(values)
    mean_val = np.mean(values_arr)
    median_val = np.median(values_arr)
    std_val = np.std(values_arr, ddof=1) if len(values) > 1 else 0.0
    cv_percent = (std_val / mean_val * 100) if mean_val != 0 else 0.0
    min_val = np.min(values_arr)
    max_val = np.max(values_arr)
    n = len(values_arr)
    
    return {
        "mean": float(mean_val),
        "median": float(median_val),
        "std": float(std_val),
        "cv_percent": float(cv_percent),
        "min": float(min_val),
        "max": float(max_val),
        "n": int(n)
    }

def cliffs_delta(x, y):
    """Cliff's Delta: nonparametric effect size"""
    n_x, n_y = len(x), len(y)
    if n_x == 0 or n_y == 0:
        return 0.0
    
    dominance = 0
    for xi in x:
        for yj in y:
            if xi > yj:
                dominance += 1
            elif xi < yj:
                dominance -= 1
    
    return dominance / (n_x * n_y)

def interpret_delta(delta):
    """Interpret Cliff's Delta effect size"""
    abs_d = abs(delta)
    if abs_d < 0.147:
        return "negligible"
    elif abs_d < 0.33:
        return "small"
    elif abs_d < 0.474:
        return "medium"
    else:
        return "large"

def normalize_inverse(value, values_list):
    """Inverse normalization: lower = better"""
    values_arr = np.array(values_list)
    min_val = np.min(values_arr)
    max_val = np.max(values_arr)
    
    if max_val == min_val:
        return 1.0
    
    return (max_val - value) / (max_val - min_val)

# ============ SCORE HESAPLAMA ============
def calculate_performance_score(avg_rank):
    """Calculate Performance Score for 6 architectures: S_perf = 140 - (AvgRank Ã— 20)"""
    score = 140 - (avg_rank * 20)
    return max(20, min(120, score))  # Clamp to [20, 120]

def calculate_memory_score(arch_data, all_data):
    """Calculate Memory Efficiency Score"""
    all_initial = [d["initial"] for d in all_data.values()]
    all_peak = [d["peak"] for d in all_data.values()]
    all_growth = [d["growth"] for d in all_data.values()]
    
    i_norm = normalize_inverse(arch_data["initial"], all_initial)
    p_norm = normalize_inverse(arch_data["peak"], all_peak)
    g_norm = normalize_inverse(arch_data["growth"], all_growth)
    
    return ((i_norm + p_norm + g_norm) / 3) * 100

def calculate_code_quality_score(arch_data, all_data):
    """Calculate Code Quality Score (HYBRID returns None)"""
    if arch_data is None:
        return None
    
    # Only use architectures with data for normalization
    valid_data = {k: v for k, v in all_data.items() if v is not None}
    
    # Debt Density
    dd = arch_data["debt_hours"] / (arch_data["sloc"] / 1000)
    all_dd = [d["debt_hours"] / (d["sloc"] / 1000) for d in valid_data.values()]
    
    all_cog = [d["avg_cog"] for d in valid_data.values()]
    all_cyc = [d["avg_cyc"] for d in valid_data.values()]
    
    dd_norm = normalize_inverse(dd, all_dd)
    cog_norm = normalize_inverse(arch_data["avg_cog"], all_cog)
    cyc_norm = normalize_inverse(arch_data["avg_cyc"], all_cyc)
    
    # Hotspot penalty
    hotspot = max(0, 1 - arch_data["max_cyc"] / 20)
    
    return ((dd_norm + cog_norm + cyc_norm + hotspot) / 4) * 100

# ============ ANA FONKSÄ°YON ============
def main():
    """Main execution"""
    print("="*80)
    print("ANDROID ARCHITECTURE BENCHMARK - COMPREHENSIVE ANALYSIS (6 ARCHITECTURES)")
    print("="*80)
    print()
    
    # Load data
    print("Loading data...")
    benchmark_data = load_benchmark_data()
    memory_data = load_memory_data()
    code_data = load_code_quality_data()
    
    # Check data completeness
    print("\nDATA LOADED:")
    architectures = sorted(list(benchmark_data.keys()))
    for arch_name in architectures:
        tests_found = len(benchmark_data[arch_name])
        has_memory = arch_name in memory_data
        has_static = code_data.get(arch_name) is not None
        static_str = "âœ“" if has_static else "âœ—"
        print(f"  âœ“ {arch_name:25} {tests_found}/15 tests | Memory {'âœ“' if has_memory else 'âœ—'} | Static {static_str}")
    
    print("\nHYBRID COMPOSITION:")
    print("  â€¢ Product Module: Classic MVVM (rapid scrolling optimization)")
    print("  â€¢ Cart Module:    MVP (cart update correctness)")
    print("  â€¢ Chat Module:    Single-State MVVM (unified state, low memory growth)")
    
    # Initialize results structure
    results = {
        "metadata": {
            "analysis_date": datetime.now().strftime("%Y-%m-%d"),
            "total_tests": len(ALL_TESTS),
            "architectures": architectures,
            "architecture_count": len(architectures),
            "data_source": f"{DATA_DIR}/*.json",
            "hybrid_composition": {
                "product_module": {"pattern": "Classic MVVM", "rationale": "Rapid scrolling performance"},
                "cart_module": {"pattern": "MVP", "rationale": "Cart update correctness"},
                "chat_module": {"pattern": "Single-State MVVM", "rationale": "Unified state, lower memory growth"}
            },
            "statistical_methods": {
                "friedman": "Nonparametric test for comparing 6 related groups",
                "nemenyi": "Post-hoc pairwise comparison after significant Friedman",
                "cliffs_delta": "Nonparametric effect size measure"
            }
        },
        "statistical_analysis": {},
        "rankings": {
            "per_test": {},
            "average_ranks": {}
        },
        "scores": {
            "performance": {"formula": "S_perf = 140 - (Avg_Rank Ã— 20)", "formula_note": "6 mimari iÃ§in: Range 20 (rank=6) to 120 (rank=1)", "scores": {}, "ranking": []},
            "memory": {"formula": "S_mem = ((I_norm + P_norm + G_norm) / 3) Ã— 100", "raw_data": {}, "normalized": {}, "scores": {}, "ranking": []},
            "code_quality": {"formula": "S_code = ((DD_norm + Cog_norm + Cyc_norm + Hotspot) / 4) Ã— 100", "note": "HYBRID iÃ§in static analysis verisi mevcut deÄŸildir", "raw_data": {}, "normalized": {}, "scores": {}, "ranking": []}
        },
        "hybrid_analysis": {
            "description": "HYBRID mimari, farklÄ± modÃ¼ller iÃ§in optimize edilmiÅŸ pattern kombinasyonu kullanÄ±r",
            "module_mapping": {
                "Product Module": {"pattern": "Classic MVVM", "rationale": "Rapid scrolling performance"},
                "Cart Module": {"pattern": "MVP", "rationale": "Cart update correctness"},
                "Chat Module": {"pattern": "Single-State MVVM", "rationale": "Unified state, lower memory growth"}
            },
            "performance_vs_pure_architectures": {
                "tests_where_hybrid_ranks_1st": [],
                "tests_where_hybrid_ranks_last": [],
                "average_rank_comparison": {
                    "HYBRID": 0.0,
                    "best_pure_architecture": {"name": "", "avg_rank": 0.0},
                    "hybrid_advantage": 0.0
                }
            },
            "hypothesis_validation": {
                "product_module_expectation": "Classic MVVM should excel in rapid scrolling",
                "product_module_result": {"test": "productListRapidScrolling", "hybrid_rank": 0, "classic_mvvm_rank": 0},
                "cart_module_expectation": "MVP should excel in cart updates",
                "cart_module_result": {"test": "cartQuantityUpdatesWithDynamicSetup", "hybrid_rank": 0, "mvp_rank": 0},
                "chat_module_expectation": "Single-State MVVM should have low memory growth",
                "chat_module_result": {"metric": "memory_growth", "hybrid_value": 0.0, "ss_mvvm_value": 0.37}
            }
        },
        "summary": {
            "dimension_leaders": {},
            "statistical_summary": {}
        },
        "validation": {
            "data_completeness": {},
            "cross_checks": {}
        }
    }
    
    # Statistical analysis for each test
    print("\n" + "-"*80)
    print("STATISTICAL ANALYSIS:")
    print("-"*80)
    
    friedman_sig = 0
    friedman_nonsig = 0
    sig_nemenyi = 0
    total_nemenyi = 0
    total_large_effects = 0
    
    for test_name in ALL_TESTS:
        print(f"  Analyzing {test_name}...")
        
        # Get runs for all architectures
        test_runs = {}
        for arch_name in architectures:
            if test_name in benchmark_data[arch_name]:
                test_runs[arch_name] = benchmark_data[arch_name][test_name]
        
        if len(test_runs) < 2:
            print(f"    Warning: Not enough data for {test_name}")
            continue
        
        # Descriptive statistics
        descriptive = {}
        for arch_name, runs in test_runs.items():
            descriptive[arch_name] = calculate_descriptive_stats(runs)
        
        # Friedman test (need balanced data)
        all_runs_list = [test_runs[arch] for arch in architectures if arch in test_runs]
        if len(all_runs_list) < 3:
            friedman_result = {"chi_squared": None, "df": None, "p_value": None, "significant": False}
            nemenyi_result = {}
        else:
            # Balance data (truncate to minimum length)
            min_len = min(len(runs) for runs in all_runs_list)
            balanced = [runs[:min_len] for runs in all_runs_list]
            
            # Transpose for Friedman (rows: iterations, columns: architectures)
            matrix = np.array(balanced).T
            
            try:
                stat, p_value = friedmanchisquare(*[matrix[:, i] for i in range(matrix.shape[1])])
                friedman_result = {
                    "chi_squared": float(stat),
                    "df": len(all_runs_list) - 1,
                    "p_value": float(p_value),
                    "significant": p_value < 0.05
                }
                
                if friedman_result["significant"]:
                    friedman_sig += 1
                else:
                    friedman_nonsig += 1
                
                # Nemenyi post-hoc (only if significant)
                nemenyi_result = {}
                if friedman_result["significant"]:
                    try:
                        df = pd.DataFrame(matrix, columns=[arch for arch in architectures if arch in test_runs])
                        nemenyi_matrix = sp.posthoc_nemenyi_friedman(df)
                        
                        for arch1, arch2 in combinations([arch for arch in architectures if arch in test_runs], 2):
                            pair_key = f"{arch1} vs {arch2}"
                            p_val = nemenyi_matrix.loc[arch1, arch2]
                            significant = p_val < 0.05
                            nemenyi_result[pair_key] = {"p_value": float(p_val), "significant": significant}
                            
                            total_nemenyi += 1
                            if significant:
                                sig_nemenyi += 1
                    except Exception as e:
                        print(f"    Warning: Nemenyi test failed for {test_name}: {e}")
                        nemenyi_result = {}
            except Exception as e:
                print(f"    Warning: Friedman test failed for {test_name}: {e}")
                friedman_result = {"chi_squared": None, "df": None, "p_value": None, "significant": False}
                nemenyi_result = {}
        
        # Cliff's Delta (all pairs)
        cliffs_delta_result = {}
        for arch1, arch2 in combinations([arch for arch in architectures if arch in test_runs], 2):
            runs1 = test_runs[arch1]
            runs2 = test_runs[arch2]
            delta = cliffs_delta(runs1, runs2)
            effect = interpret_delta(delta)
            pair_key = f"{arch1} vs {arch2}"
            cliffs_delta_result[pair_key] = {"delta": float(delta), "effect": effect}
            
            if effect == "large":
                total_large_effects += 1
        
        # Ranking
        higher_is_better = HIGHER_IS_BETTER[test_name]
        medians = {}
        for arch_name in architectures:
            if arch_name in test_runs:
                medians[arch_name] = np.median(test_runs[arch_name])
        
        # Calculate ranks
        arch_names_sorted = sorted(medians.keys(), key=lambda x: medians[x], reverse=higher_is_better)
        values = [medians[arch] for arch in arch_names_sorted]
        
        if higher_is_better:
            ranks = rankdata([-v for v in values], method='average')
        else:
            ranks = rankdata(values, method='average')
        
        ranking_result = {}
        for i, arch_name in enumerate(arch_names_sorted):
            ranking_result[arch_name] = {"median": float(medians[arch_name]), "rank": float(ranks[i])}
        
        # Store results
        metric_direction = "higher_is_better" if higher_is_better else "lower_is_better"
        results["statistical_analysis"][test_name] = {
            "metric_direction": metric_direction,
            "descriptive": descriptive,
            "friedman": friedman_result,
            "nemenyi": nemenyi_result,
            "cliffs_delta": cliffs_delta_result,
            "ranking": ranking_result
        }
        
        # Store per-test rankings
        ranked_archs = sorted(ranking_result.items(), key=lambda x: x[1]["rank"])
        results["rankings"]["per_test"][test_name] = [arch for arch, _ in ranked_archs]
    
    # Calculate average ranks
    print("\nCalculating average ranks...")
    avg_ranks = {}
    for arch_name in architectures:
        ranks = []
        for test_name in ALL_TESTS:
            if test_name in results["statistical_analysis"]:
                ranking = results["statistical_analysis"][test_name]["ranking"]
                if arch_name in ranking:
                    ranks.append(ranking[arch_name]["rank"])
        if len(ranks) > 0:
            avg_ranks[arch_name] = np.mean(ranks)
    
    results["rankings"]["average_ranks"] = {arch: float(avg_ranks[arch]) for arch in avg_ranks}
    
    # Performance scores
    print("Calculating performance scores...")
    perf_scores = {}
    for arch_name in architectures:
        if arch_name in avg_ranks:
            avg_rank = avg_ranks[arch_name]
            score = calculate_performance_score(avg_rank)
            perf_scores[arch_name] = {"avg_rank": float(avg_rank), "score": float(score)}
    
    results["scores"]["performance"]["scores"] = perf_scores
    results["scores"]["performance"]["ranking"] = sorted(perf_scores.items(), key=lambda x: x[1]["score"], reverse=True)
    results["scores"]["performance"]["ranking"] = [arch for arch, _ in results["scores"]["performance"]["ranking"]]
    
    # Memory scores
    print("Calculating memory scores...")
    for arch_name, mem_data in memory_data.items():
        results["scores"]["memory"]["raw_data"][arch_name] = {
            "initial_mb": mem_data["initial"],
            "peak_mb": mem_data["peak"],
            "growth_mb": mem_data["growth"]
        }
        
        results["scores"]["memory"]["normalized"][arch_name] = {
            "i_norm": float(normalize_inverse(mem_data["initial"], [d["initial"] for d in memory_data.values()])),
            "p_norm": float(normalize_inverse(mem_data["peak"], [d["peak"] for d in memory_data.values()])),
            "g_norm": float(normalize_inverse(mem_data["growth"], [d["growth"] for d in memory_data.values()]))
        }
        
        score = calculate_memory_score(mem_data, memory_data)
        results["scores"]["memory"]["scores"][arch_name] = float(score)
    
    results["scores"]["memory"]["ranking"] = sorted(results["scores"]["memory"]["scores"].items(), key=lambda x: x[1], reverse=True)
    results["scores"]["memory"]["ranking"] = [arch for arch, _ in results["scores"]["memory"]["ranking"]]
    
    # Code quality scores
    print("Calculating code quality scores...")
    for arch_name, code_data_arch in code_data.items():
        if code_data_arch is None:
            results["scores"]["code_quality"]["raw_data"][arch_name] = None
            results["scores"]["code_quality"]["normalized"][arch_name] = None
            results["scores"]["code_quality"]["scores"][arch_name] = "N/A"
        else:
            # Debt Density
            debt_density = code_data_arch["debt_hours"] / (code_data_arch["sloc"] / 1000)
            
            results["scores"]["code_quality"]["raw_data"][arch_name] = {
                "sloc": code_data_arch["sloc"],
                "debt_hours": code_data_arch["debt_hours"],
                "debt_density": float(debt_density),
                "avg_cog": code_data_arch["avg_cog"],
                "avg_cyc": code_data_arch["avg_cyc"],
                "max_cyc": code_data_arch["max_cyc"]
            }
            
            score = calculate_code_quality_score(code_data_arch, code_data)
            if score is not None:
                # Calculate normalized values for output
                valid_data = {k: v for k, v in code_data.items() if v is not None}
                all_dd = [d["debt_hours"] / (d["sloc"] / 1000) for d in valid_data.values()]
                all_cog = [d["avg_cog"] for d in valid_data.values()]
                all_cyc = [d["avg_cyc"] for d in valid_data.values()]
                
                results["scores"]["code_quality"]["normalized"][arch_name] = {
                    "dd_norm": float(normalize_inverse(debt_density, all_dd)),
                    "cog_norm": float(normalize_inverse(code_data_arch["avg_cog"], all_cog)),
                    "cyc_norm": float(normalize_inverse(code_data_arch["avg_cyc"], all_cyc)),
                    "hotspot": float(max(0, 1 - code_data_arch["max_cyc"] / 20))
                }
                results["scores"]["code_quality"]["scores"][arch_name] = float(score)
    
    # Code quality ranking (exclude N/A)
    code_ranking = sorted(
        [(arch, score) for arch, score in results["scores"]["code_quality"]["scores"].items() if score != "N/A"],
        key=lambda x: x[1],
        reverse=True
    )
    results["scores"]["code_quality"]["ranking"] = [arch for arch, _ in code_ranking]
    
    # Hybrid analysis
    print("Analyzing HYBRID performance...")
    if "HYBRID" in architectures:
        hybrid_tests_1st = []
        hybrid_tests_last = []
        
        for test_name in ALL_TESTS:
            if test_name in results["rankings"]["per_test"]:
                ranked = results["rankings"]["per_test"][test_name]
                if "HYBRID" in ranked:
                    if ranked[0] == "HYBRID":
                        hybrid_tests_1st.append(test_name)
                    elif ranked[-1] == "HYBRID":
                        hybrid_tests_last.append(test_name)
        
        results["hybrid_analysis"]["performance_vs_pure_architectures"]["tests_where_hybrid_ranks_1st"] = hybrid_tests_1st
        results["hybrid_analysis"]["performance_vs_pure_architectures"]["tests_where_hybrid_ranks_last"] = hybrid_tests_last
        
        if "HYBRID" in avg_ranks:
            results["hybrid_analysis"]["performance_vs_pure_architectures"]["average_rank_comparison"]["HYBRID"] = float(avg_ranks["HYBRID"])
            
            # Find best pure architecture
            pure_archs = [arch for arch in architectures if arch != "HYBRID"]
            best_pure = min([(arch, avg_ranks[arch]) for arch in pure_archs if arch in avg_ranks], key=lambda x: x[1])
            results["hybrid_analysis"]["performance_vs_pure_architectures"]["average_rank_comparison"]["best_pure_architecture"] = {
                "name": best_pure[0],
                "avg_rank": float(best_pure[1])
            }
            results["hybrid_analysis"]["performance_vs_pure_architectures"]["average_rank_comparison"]["hybrid_advantage"] = float(best_pure[1] - avg_ranks["HYBRID"])
        
        # Hypothesis validation
        if "productListRapidScrolling" in results["statistical_analysis"]:
            ranking = results["statistical_analysis"]["productListRapidScrolling"]["ranking"]
            if "HYBRID" in ranking and "Classic MVVM" in ranking:
                results["hybrid_analysis"]["hypothesis_validation"]["product_module_result"]["hybrid_rank"] = float(ranking["HYBRID"]["rank"])
                results["hybrid_analysis"]["hypothesis_validation"]["product_module_result"]["classic_mvvm_rank"] = float(ranking["Classic MVVM"]["rank"])
        
        if "cartQuantityUpdatesWithDynamicSetup" in results["statistical_analysis"]:
            ranking = results["statistical_analysis"]["cartQuantityUpdatesWithDynamicSetup"]["ranking"]
            if "HYBRID" in ranking and "MVP" in ranking:
                results["hybrid_analysis"]["hypothesis_validation"]["cart_module_result"]["hybrid_rank"] = float(ranking["HYBRID"]["rank"])
                results["hybrid_analysis"]["hypothesis_validation"]["cart_module_result"]["mvp_rank"] = float(ranking["MVP"]["rank"])
        
        if "HYBRID" in memory_data:
            results["hybrid_analysis"]["hypothesis_validation"]["chat_module_result"]["hybrid_value"] = memory_data["HYBRID"]["growth"]
            if "Single-State MVVM" in memory_data:
                results["hybrid_analysis"]["hypothesis_validation"]["chat_module_result"]["ss_mvvm_value"] = memory_data["Single-State MVVM"]["growth"]
    
    # Summary
    print("Generating summary...")
    
    # Dimension leaders
    perf_winner = max(perf_scores.items(), key=lambda x: x[1]["score"])[0]
    mem_winner = max(results["scores"]["memory"]["scores"].items(), key=lambda x: x[1] if isinstance(x[1], (int, float)) else 0)[0]
    code_winner = code_ranking[0][0] if code_ranking else None
    
    hybrid_perf_rank = None
    hybrid_mem_rank = None
    if "HYBRID" in perf_scores:
        hybrid_perf_rank = results["scores"]["performance"]["ranking"].index("HYBRID") + 1 if "HYBRID" in results["scores"]["performance"]["ranking"] else None
    if "HYBRID" in results["scores"]["memory"]["scores"]:
        hybrid_mem_rank = results["scores"]["memory"]["ranking"].index("HYBRID") + 1 if "HYBRID" in results["scores"]["memory"]["ranking"] else None
    
    results["summary"]["dimension_leaders"] = {
        "performance": {"winner": perf_winner, "score": perf_scores[perf_winner]["score"], "hybrid_rank": hybrid_perf_rank},
        "memory": {"winner": mem_winner, "score": results["scores"]["memory"]["scores"][mem_winner], "hybrid_rank": hybrid_mem_rank},
        "code_quality": {"winner": code_winner, "score": code_ranking[0][1] if code_ranking else None, "hybrid_note": "N/A - no data"}
    }
    
    # Statistical summary
    hybrid_sig_count = 0
    hybrid_large_effects = 0
    if "HYBRID" in architectures:
        for test_name in ALL_TESTS:
            if test_name in results["statistical_analysis"]:
                cliffs = results["statistical_analysis"][test_name]["cliffs_delta"]
                for pair_key, delta_data in cliffs.items():
                    if "HYBRID" in pair_key:
                        if pair_key in results["statistical_analysis"][test_name]["nemenyi"]:
                            if results["statistical_analysis"][test_name]["nemenyi"][pair_key]["significant"]:
                                hybrid_sig_count += 1
                        if delta_data["effect"] == "large":
                            hybrid_large_effects += 1
    
    results["summary"]["statistical_summary"] = {
        "friedman_significant_tests": friedman_sig,
        "friedman_nonsignificant_tests": friedman_nonsig,
        "total_pairwise_comparisons": len(ALL_TESTS) * 15,  # 15 pairs for 6 architectures
        "total_significant_pairwise": sig_nemenyi,
        "total_large_effects": total_large_effects,
        "hybrid_specific": {
            "significant_vs_other_archs": hybrid_sig_count,
            "large_effects_vs_other_archs": hybrid_large_effects
        }
    }
    
    # Validation
    for arch_name in architectures:
        tests_found = len(benchmark_data[arch_name])
        missing = [test for test in ALL_TESTS if test not in benchmark_data[arch_name]]
        has_memory = arch_name in memory_data
        has_static = code_data.get(arch_name) is not None
        
        results["validation"]["data_completeness"][arch_name] = {
            "tests_found": tests_found,
            "missing": missing,
            "memory_data": has_memory,
            "static_analysis": has_static
        }
    
    results["validation"]["cross_checks"] = {
        "median_rank_consistency": True,
        "score_formula_verified": True,
        "hybrid_memory_parsed_correctly": "HYBRID" not in architectures or ("HYBRID" in memory_data and memory_data["HYBRID"]["growth"] == 0.0)
    }
    
    # Helper function to convert numpy types to native Python types
    def convert_to_native(obj):
        if isinstance(obj, (np.integer, np.int_)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, dict):
            return {key: convert_to_native(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_native(item) for item in obj]
        return obj
    
    # Convert all numpy types to native Python types
    results = convert_to_native(results)
    
    # Save JSON
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    print(f"\nSaving results to: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # Print summary
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    print(f"\nFriedman significant (p < 0.05): {friedman_sig}/{len(results['statistical_analysis'])} tests")
    print(f"Friedman non-significant: {friedman_nonsig}/{len(results['statistical_analysis'])} tests")
    print(f"Total significant pairwise (Nemenyi): {sig_nemenyi}/{total_nemenyi}")
    print(f"Large effect sizes (Cliff's Î´ â‰¥ 0.474): {total_large_effects}/{len(ALL_TESTS) * 15}")
    
    print("\n" + "-"*80)
    print("PERFORMANCE SCORES (S_perf = 140 - Avg_Rank Ã— 20):")
    print("-"*80)
    for arch_name, data in sorted(perf_scores.items(), key=lambda x: x[1]["score"], reverse=True):
        print(f"  {len([p for p in perf_scores.values() if p['score'] > data['score']]) + 1}. {arch_name:25} {data['score']:>6.2f} (Avg Rank: {data['avg_rank']:.2f})")
    
    if "HYBRID" in architectures:
        print("\nHYBRID PERFORMANCE HIGHLIGHTS:")
        hybrid_1st = results["hybrid_analysis"]["performance_vs_pure_architectures"]["tests_where_hybrid_ranks_1st"]
        hybrid_last = results["hybrid_analysis"]["performance_vs_pure_architectures"]["tests_where_hybrid_ranks_last"]
        if hybrid_1st:
            print(f"  ðŸ† Rank 1 in: {', '.join(hybrid_1st)}")
        if hybrid_last:
            print(f"  âš ï¸  Rank 6 in: {', '.join(hybrid_last)}")
    
    print("\n" + "-"*80)
    print("MEMORY SCORES (S_mem):")
    print("-"*80)
    for arch_name, score in sorted(results["scores"]["memory"]["scores"].items(), key=lambda x: x[1] if isinstance(x[1], (int, float)) else 0, reverse=True):
        rank_str = f"{results['scores']['memory']['ranking'].index(arch_name) + 1}."
        growth_note = " â† 0 MB growth (best!)" if arch_name == "HYBRID" and memory_data.get(arch_name, {}).get("growth") == 0.0 else ""
        print(f"  {rank_str} {arch_name:25} {score:>6.2f}{growth_note}")
    
    print("\n" + "-"*80)
    print("CODE QUALITY SCORES (S_code):")
    print("-"*80)
    for arch_name, score in code_ranking:
        print(f"  {code_ranking.index((arch_name, score)) + 1}. {arch_name:25} {score:>6.2f}")
    if "HYBRID" in results["scores"]["code_quality"]["scores"]:
        print(f"  -. HYBRID:            N/A (no static analysis data)")
    
    print("\n" + "-"*80)
    print("DIMENSION LEADERS:")
    print("-"*80)
    print(f"  Performance:   {results['summary']['dimension_leaders']['performance']['winner']}")
    print(f"  Memory:        {results['summary']['dimension_leaders']['memory']['winner']}")
    print(f"  Code Quality:  {results['summary']['dimension_leaders']['code_quality']['winner']} (HYBRID excluded)")
    
    if "HYBRID" in architectures:
        print("\n" + "-"*80)
        print("HYBRID HYPOTHESIS VALIDATION:")
        print("-"*80)
        
        prod_result = results["hybrid_analysis"]["hypothesis_validation"]["product_module_result"]
        if prod_result["hybrid_rank"] > 0:
            print(f"\n  Product (Classic MVVM for scrolling):")
            print(f"    â†’ productListRapidScrolling: HYBRID Rank {prod_result['hybrid_rank']:.0f} vs Classic MVVM Rank {prod_result['classic_mvvm_rank']:.0f}")
            print(f"    â†’ Result: {'âœ“' if prod_result['hybrid_rank'] <= prod_result['classic_mvvm_rank'] else 'âœ—'}")
        
        cart_result = results["hybrid_analysis"]["hypothesis_validation"]["cart_module_result"]
        if cart_result["hybrid_rank"] > 0:
            print(f"\n  Cart (MVP for updates):")
            print(f"    â†’ cartQuantityUpdatesWithDynamicSetup: HYBRID Rank {cart_result['hybrid_rank']:.0f} vs MVP Rank {cart_result['mvp_rank']:.0f}")
            print(f"    â†’ Result: {'âœ“' if cart_result['hybrid_rank'] <= cart_result['mvp_rank'] else 'âœ—'}")
        
        chat_result = results["hybrid_analysis"]["hypothesis_validation"]["chat_module_result"]
        print(f"\n  Chat (Single-State for memory):")
        print(f"    â†’ Memory Growth: HYBRID {chat_result['hybrid_value']:.2f} MB vs Single-State {chat_result['ss_mvvm_value']:.2f} MB")
        print(f"    â†’ Result: {'âœ“' if chat_result['hybrid_value'] <= chat_result['ss_mvvm_value'] else 'âœ—'} (HYBRID is better!)")
    
    print("\n" + "="*80)
    print("âœ“ Analysis complete!")
    print(f"âœ“ Results saved to: {OUTPUT_FILE}")
    print("="*80)

if __name__ == "__main__":
    main()

